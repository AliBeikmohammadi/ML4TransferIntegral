{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d210843",
   "metadata": {
    "id": "0d210843"
   },
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e808a",
   "metadata": {
    "id": "478e808a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Conv2d, Dropout, AdaptiveAvgPool2d\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch.optim import Adam\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdb382",
   "metadata": {
    "id": "e4bdb382"
   },
   "source": [
    "# Set the seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dec9e4",
   "metadata": {
    "id": "21dec9e4"
   },
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "SEED = 2\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c288b3",
   "metadata": {
    "id": "77c288b3"
   },
   "source": [
    "# Making info table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e027f",
   "metadata": {
    "id": "669e027f"
   },
   "outputs": [],
   "source": [
    "def load_npy_files(base_folder):\n",
    "    \"\"\"\n",
    "    Load all .npy file paths from the given base folder.\n",
    "    \"\"\"\n",
    "    npy_files = []\n",
    "    materials = os.listdir(base_folder)\n",
    "\n",
    "    for material in materials:\n",
    "        material_path = os.path.join(base_folder, material)\n",
    "        if not os.path.isdir(material_path):\n",
    "            continue\n",
    "\n",
    "        for condition in ['CIP', 'CM']:\n",
    "            condition_path = os.path.join(material_path, condition)\n",
    "            if not os.path.exists(condition_path):\n",
    "                continue\n",
    "\n",
    "            for file in os.listdir(condition_path):\n",
    "                if file.endswith('.gjf.npy'):\n",
    "                    npy_files.append({\n",
    "                        'material': material,\n",
    "                        'condition': condition,\n",
    "                        'file_path': os.path.join(condition_path, file),\n",
    "                        'filename': file.split('.')[0]  # Extract filename without extension\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(npy_files)\n",
    "\n",
    "\n",
    "def load_csv_targets(base_folder):\n",
    "    \"\"\"\n",
    "    Load target values from the transfer_integrals.csv files and create a dataframe.\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    materials = os.listdir(base_folder)\n",
    "\n",
    "    for material in materials:\n",
    "        material_path = os.path.join(base_folder, material)\n",
    "        if not os.path.isdir(material_path):\n",
    "            continue\n",
    "\n",
    "        csv_file_path = os.path.join(material_path, 'transfer_integrals.csv')\n",
    "        if not os.path.exists(csv_file_path):\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        df['material'] = material\n",
    "        df['filename'] = df['Filename'].apply(lambda x: x.split('.')[0])  # Extract filename without extension\n",
    "        targets.append(df)\n",
    "\n",
    "    return pd.concat(targets, ignore_index=True)\n",
    "\n",
    "\n",
    "def find_missing_targets(base_folder):\n",
    "    \"\"\"\n",
    "    Find .npy files that are missing their target values in the CSV files.\n",
    "    \"\"\"\n",
    "    # Load all npy file paths\n",
    "    npy_files = load_npy_files(base_folder)\n",
    "\n",
    "    # Load target values from CSV files\n",
    "    target_data = load_csv_targets(base_folder)\n",
    "\n",
    "    # Extract filenames (without extensions) from the CSV targets for comparison\n",
    "    target_filenames = target_data['filename'].unique()\n",
    "\n",
    "    # Find npy files without corresponding targets\n",
    "    missing_targets = npy_files[~npy_files['filename'].isin(target_filenames)]\n",
    "\n",
    "    return missing_targets\n",
    "\n",
    "\n",
    "def prepare_dataset(base_folder, epsilon):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for use in machine learning models, excluding missing targets.\n",
    "    \"\"\"\n",
    "    # Load npy data paths\n",
    "    npy_data = load_npy_files(base_folder)\n",
    "\n",
    "    # Load target values from CSV files\n",
    "    target_data = load_csv_targets(base_folder)\n",
    "\n",
    "    # Find missing targets\n",
    "    missing_targets = find_missing_targets(base_folder)\n",
    "    if not missing_targets.empty:\n",
    "        print(\"Found .npy files with missing target values:\")\n",
    "        print(missing_targets.loc[missing_targets['condition'] == 'CIP', 'file_path'])\n",
    "    else:\n",
    "        print(\"All .npy files have corresponding target values.\")\n",
    "\n",
    "    # Filter out npy files that are missing their targets\n",
    "    npy_data_filtered = npy_data[~npy_data['filename'].isin(missing_targets['filename'])]\n",
    "\n",
    "    # Merge npy data with target values\n",
    "    dataset = npy_data_filtered.merge(target_data, on=['material', 'filename'], how='left')\n",
    "\n",
    "    # Drop rows with missing target values (this is just a safety check)\n",
    "    dataset = dataset.dropna(subset=['HOMO'])\n",
    "\n",
    "    # Compute the two target variables\n",
    "    dataset['target_HOMO'] = dataset['HOMO']\n",
    "    dataset['target_log_abs_HOMO'] = np.log(epsilon + np.abs(dataset['HOMO']))\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    dataset = dataset.drop(columns=['Filename', 'NLUMO', 'LUMO', 'HOMO', 'NHOMO'])\n",
    "\n",
    "    # Separate CIP and CM data\n",
    "    dataset_CIP = dataset[dataset['condition'] == 'CIP'].rename(columns={'file_path': 'file_path_CIP'})\n",
    "    dataset_CM = dataset[dataset['condition'] == 'CM'].rename(columns={'file_path': 'file_path_CM'})\n",
    "\n",
    "    # Merge CIP and CM datasets based on material and filename\n",
    "    merged_dataset = pd.merge(dataset_CIP[['material', 'filename', 'file_path_CIP', 'target_HOMO', 'target_log_abs_HOMO']],\n",
    "                              dataset_CM[['material', 'filename', 'file_path_CM']],\n",
    "                              on=['material', 'filename'])\n",
    "\n",
    "    return merged_dataset\n",
    "\n",
    "def get_data(base_folder, target, input_type, test_material, SEED, epsilon):\n",
    "    \"\"\"\n",
    "    Prepare the dataset ready for training based on the selected target and input type.\n",
    "\n",
    "    Parameters:\n",
    "    - base_folder: str, the base directory containing data files.\n",
    "    - target: str, the target column to use ('target_HOMO' or 'target_log_abs_HOMO').\n",
    "    - input_type: str, the input type ('CIP', 'CM', or 'Multi').\n",
    "    - test_material: str, optional material to use as a test set.\n",
    "    \"\"\"\n",
    "    merged_dataset = prepare_dataset(base_folder, epsilon)\n",
    "\n",
    "    # Choose the target column\n",
    "    if target not in ['target_HOMO', 'target_log_abs_HOMO']:\n",
    "        raise ValueError(\"Invalid target. Choose 'target_HOMO' or 'target_log_abs_HOMO'.\")\n",
    "\n",
    "    # Select the input type\n",
    "    if input_type == 'CIP':\n",
    "        merged_dataset = merged_dataset[['material', 'filename', 'file_path_CIP', target]]\n",
    "    elif input_type == 'CM':\n",
    "        merged_dataset = merged_dataset[['material', 'filename', 'file_path_CM', target]]\n",
    "    elif input_type == 'Multi':\n",
    "        merged_dataset = merged_dataset[['material', 'filename', 'file_path_CIP', 'file_path_CM', target]]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input type. Choose 'CIP', 'CM', or 'Multi'.\")\n",
    "\n",
    "    # Split data into train/test sets\n",
    "    if test_material:\n",
    "        train_data = merged_dataset[merged_dataset['material'] != test_material]\n",
    "        test_data = merged_dataset[merged_dataset['material'] == test_material]\n",
    "    else:\n",
    "        train_data, test_data = train_test_split(\n",
    "            merged_dataset,\n",
    "            test_size=0.2,\n",
    "            stratify=merged_dataset['material'],\n",
    "            random_state=SEED\n",
    "        )\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e4131",
   "metadata": {
    "id": "988e4131"
   },
   "source": [
    "# Plots and anlaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e738a5e",
   "metadata": {
    "id": "6e738a5e"
   },
   "outputs": [],
   "source": [
    "def plot_distribution(data, column, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data[column], kde=True, color='blue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def analyze_data(data, dataset):\n",
    "    print(data.describe())  # Basic descriptive statistics\n",
    "    plt.figure()\n",
    "    plt.hist(data['material'], color='blue')\n",
    "    plt.title(f'Material Distribution on {dataset}')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'train_data' and 'test_data' are your datasets\n",
    "def analyze_target_distribution(train_data, test_data, target='target_log_abs_HOMO'):\n",
    "    analyze_data(train_data, 'Train Set')\n",
    "    analyze_data(test_data, 'Test Set')\n",
    "    plot_distribution(train_data, target, f'Distribution of {target} in Training Data')\n",
    "    plot_distribution(test_data, target, f'Distribution of {target} in Testing Data')\n",
    "\n",
    "\n",
    "def plot_predictions(data_loader, model_path, epsilon):\n",
    "    actuals, predictions = [], []\n",
    "    extracted_features = extract_features_from_path(model_path)\n",
    "    model_type= extracted_features['model_type']\n",
    "    input_type= extracted_features['input_type']\n",
    "    target_type = extracted_features['target']\n",
    "    model = load_model(model_path)\n",
    "    for batch_idx, data in enumerate(data_loader):\n",
    "        output, actual  = predict_model(data, model, model_type, input_type)\n",
    "        actuals.extend(actual.cpu().numpy())\n",
    "        predictions.extend(output.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    # Check target type and compute the counterpart if necessary\n",
    "    if target_type == \"target_log_abs_HOMO\":\n",
    "        actuals_transformed = np.exp(actuals) - epsilon\n",
    "        predictions_transformed = np.exp(predictions) - epsilon\n",
    "        transformed_title = 'HOMO'\n",
    "        title = \"log_abs_HOMO\"\n",
    "    elif target_type == \"target_HOMO\":\n",
    "        actuals_transformed = np.log(epsilon + np.abs(actuals))\n",
    "        predictions_transformed = np.log(epsilon + np.abs(predictions))\n",
    "        transformed_title = 'log_abs_HOMO'\n",
    "        title = 'HOMO'\n",
    "\n",
    "    # Compute metrics for original target\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "\n",
    "    # Compute metrics for transformed target\n",
    "    mse_transformed = mean_squared_error(actuals_transformed, predictions_transformed)\n",
    "    rmse_transformed = np.sqrt(mse_transformed)\n",
    "    mae_transformed = mean_absolute_error(actuals_transformed, predictions_transformed)\n",
    "    r2_transformed = r2_score(actuals_transformed, predictions_transformed)\n",
    "\n",
    "\n",
    "    # Plotting for original target\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)  # Subplot for original target\n",
    "    plt.scatter(actuals, predictions, facecolors='none', edgecolors='b', alpha=0.5)\n",
    "    plt.xlabel(f'Actual {title}')\n",
    "    plt.ylabel(f'Estimated {title}')\n",
    "    plt.title(f\"{model_type} Model Trained on {title}\")\n",
    "    plt.plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], 'k')\n",
    "    plt.grid(True)\n",
    "    plt.text(min(actuals), max(predictions), f'MSE: {mse:.2f}\\nRMSE: {rmse:.2f}\\nMAE: {mae:.2f}\\nR²: {r2:.2f}',\n",
    "             verticalalignment='top', horizontalalignment='left', color='red')\n",
    "\n",
    "    # Plotting for transformed target\n",
    "    plt.subplot(1, 2, 2)  # Subplot for transformed target\n",
    "    plt.scatter(actuals_transformed, predictions_transformed, facecolors='none', edgecolors='b', alpha=0.5)\n",
    "    plt.xlabel(f'Actual {transformed_title}')\n",
    "    plt.ylabel(f'Estimated {transformed_title}')\n",
    "    plt.title(f\"{model_type} Model Trained on {title}\")\n",
    "    plt.plot([min(actuals_transformed), max(actuals_transformed)], [min(actuals_transformed), max(actuals_transformed)], 'k')\n",
    "    plt.grid(True)\n",
    "    plt.text(min(actuals_transformed), max(predictions_transformed),\n",
    "             f'MSE: {mse_transformed:.2f}\\nRMSE: {rmse_transformed:.2f}\\nMAE: {mae_transformed:.2f}\\nR²: {r2_transformed:.2f}',\n",
    "             verticalalignment='top', horizontalalignment='left', color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print metrics for both targets\n",
    "    print(f\"Original ({title}) - MSE: {mse:.2f} | RMSE: {rmse:.2f} | MAE: {mae:.2f} | R²: {r2:.2f}\")\n",
    "    print(f\"Transformed ({transformed_title}) - MSE: {mse_transformed:.2f} | RMSE: {rmse_transformed:.2f} | MAE: {mae_transformed:.2f} | R²: {r2_transformed:.2f}\")\n",
    "\n",
    "\n",
    "def plot_matrix(matrix, title=\"Matrix\"):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(matrix, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def find_actual_homo(base_folder, sample_file_path, epsilon):\n",
    "    \"\"\"\n",
    "    Find the actual HOMO value for a given sample file path.\n",
    "\n",
    "    Args:\n",
    "    base_folder (str): Base folder where data folders are located.\n",
    "    sample_file_path (str): Full path to the sample file.\n",
    "\n",
    "    Returns:\n",
    "    float: The actual HOMO value, or None if not found.\n",
    "    \"\"\"\n",
    "    # Load all targets into a DataFrame\n",
    "    df_targets = load_csv_targets(base_folder)\n",
    "\n",
    "    # Extract the material and filename from the sample file path\n",
    "    parts = sample_file_path.split('/')\n",
    "    material = parts[-3]\n",
    "    filename = parts[-1].split('.')[0]\n",
    "\n",
    "    # Find the row in the DataFrame\n",
    "    row = df_targets[(df_targets['material'] == material) & (df_targets['filename'] == filename)]\n",
    "    if not row.empty:\n",
    "        return row['HOMO'].values[0], np.log(epsilon+ np.abs(row['HOMO'].values[0]))  # Assuming the column with the HOMO value is named 'HOMO'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Assuming 'model' is your pre-trained model instance\n",
    "def predict_and_visualize(model_path, sample_path_cip, sample_path_cm, epsilon):\n",
    "    parts = sample_path_cip.split('/')\n",
    "    base_folder = os.path.join(parts[0], parts[1])\n",
    "    extracted_features = extract_features_from_path(model_path)\n",
    "    input_type= extracted_features['input_type']\n",
    "\n",
    "    # Load the sample data\n",
    "    sample_cip = np.load(sample_path_cip)\n",
    "    sample_cm = np.load(sample_path_cm)\n",
    "\n",
    "    print('Sample Addresses: \\n', sample_path_cip,'\\n', sample_path_cm)\n",
    "    # Visualize the matrices\n",
    "    plot_matrix(sample_cip, title=\"CIP Data Sample\")\n",
    "    plot_matrix(sample_cm, title=\"CM Data Sample\")\n",
    "\n",
    "    # Convert numpy array to torch tensor\n",
    "    sample_cip_tensor = torch.tensor(sample_cip, dtype=torch.float32)\n",
    "    sample_cm_tensor = torch.tensor(sample_cm, dtype=torch.float32)\n",
    "\n",
    "    # Add a batch dimension (model expects batches)\n",
    "    sample_cip_tensor = sample_cip_tensor.unsqueeze(0)\n",
    "    sample_cm_tensor = sample_cm_tensor.unsqueeze(0)\n",
    "\n",
    "    model = load_model(model_path)\n",
    "    print('Model Address: \\n', model_path)\n",
    "    # Ensure the model is in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # If the model is on CUDA, transfer tensor to CUDA\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        sample_cip_tensor = sample_cip_tensor.cuda()\n",
    "        sample_cm_tensor = sample_cm_tensor.cuda()\n",
    "\n",
    "    # Make a prediction\n",
    "    with torch.no_grad():\n",
    "        predict_log_abs_homo = model(sample_cip_tensor, sample_cm_tensor, input_type) #model(sample_cip_tensor)\n",
    "        predict_homo = np.exp(predict_log_abs_homo.item()-epsilon)\n",
    "\n",
    "    # Process the prediction if necessary, e.g., applying softmax or argmax, depending on the model output\n",
    "    print(\"Predicted HOMO Value:\", predict_homo.item())\n",
    "    print(\"Predicted Log Abs HOMO Value:\", predict_log_abs_homo.item())\n",
    "    actual_homo,  actual_log_abs_homo= find_actual_homo(base_folder, sample_path_cip, epsilon)\n",
    "    print(\"Actual HOMO Value:\", actual_homo)\n",
    "    print(\"Actual Log Abs HOMO Value:\", actual_log_abs_homo)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0920c1",
   "metadata": {
    "id": "5a0920c1"
   },
   "source": [
    "# Data loader, Models' architecture, Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbde2b3",
   "metadata": {
    "id": "bbbde2b3"
   },
   "outputs": [],
   "source": [
    "def create_unique_folder(base_folder, model_type, input_type, target, test_material, epochs, lr, batch_size, seed):\n",
    "    \"\"\"\n",
    "    Create a unique folder based on the provided parameters and save results.\n",
    "    \"\"\"\n",
    "    # Create a unique folder name based on the parameters\n",
    "    folder_name = f\"{model_type}_{input_type}_{target}_{test_material}_epochs{epochs}_lr{lr}_batch{batch_size}_seed{seed}\"\n",
    "    folder_path = os.path.join(base_folder, folder_name)\n",
    "    # Create the folder if it does not exist\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    return folder_name, folder_path\n",
    "\n",
    "def extract_features_from_path(folder_path):\n",
    "    \"\"\"\n",
    "    Extract model parameters from a folder name created by create_unique_folder, handling complex target names.\n",
    "\n",
    "    Args:\n",
    "    folder_path (str): The path to the folder.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing the extracted features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove the file component from the path if it exists\n",
    "    folder_path = os.path.dirname(folder_path)\n",
    "    \n",
    "    # Get the folder name from the full path\n",
    "    folder_name = os.path.basename(folder_path)\n",
    "\n",
    "    # Initialize dictionary to store the components\n",
    "    features = {\n",
    "        'model_type': None,\n",
    "        'input_type': None,\n",
    "        'target': None,\n",
    "        'test_material': None,\n",
    "        'epochs': None,\n",
    "        'lr': None,\n",
    "        'batch_size': None,\n",
    "        'SEED': None\n",
    "    }\n",
    "\n",
    "    # Extract 'seed' at the end as it's the easiest and work backwards\n",
    "    parts = folder_name.split('_seed')\n",
    "    features['SEED'] = int(parts[1])\n",
    "\n",
    "    # Split the remaining by 'batch'\n",
    "    parts = parts[0].split('_batch')\n",
    "    features['batch_size'] = int(parts[1])\n",
    "\n",
    "    # Split by 'lr'\n",
    "    parts = parts[0].split('_lr')\n",
    "    features['lr'] = float(parts[1])\n",
    "\n",
    "    # Split by 'epochs'\n",
    "    parts = parts[0].split('_epochs')\n",
    "    features['epochs'] = int(parts[1])\n",
    "\n",
    "    # Remaining part will have 'model_type', 'input_type', 'target', and 'test_material'\n",
    "    parts = parts[0].split('_')\n",
    "\n",
    "    # As the first four elements should now be model_type, input_type, target, and test_material\n",
    "    if len(parts) >= 4:\n",
    "        features['model_type'] = parts[0]\n",
    "        features['input_type'] = parts[1]\n",
    "        features['target'] = '_'.join(parts[2:-1])  # Join all parts that make up the target as it may contain underscores\n",
    "        features['test_material'] = parts[-1]\n",
    "    return features\n",
    "\n",
    "\n",
    "def save_results_to_csv(folder_name, folder_path, epoch, train_loss, mse_train_loss, mae_train_loss, mse_test_loss, mae_test_loss):\n",
    "    \"\"\"\n",
    "    Save training and test results to a CSV file inside the unique folder.\n",
    "    \"\"\"\n",
    "    csv_file = os.path.join(folder_path, folder_name+\".csv\")\n",
    "\n",
    "    # If the file doesn't exist, write the header first\n",
    "    file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Epoch\", \"Train Loss\", \"MSE Train Loss\", \"MAE Train Loss\", \"MSE Test Loss\", \"MAE Test Loss\"])\n",
    "        writer.writerow([epoch, train_loss, mse_train_loss, mae_train_loss, mse_test_loss, mae_test_loss])\n",
    "\n",
    "\n",
    "def setup_tensorboard_log(folder_name, folder_path):\n",
    "    \"\"\"\n",
    "    Setup TensorBoard SummaryWriter to log results inside the unique folder.\n",
    "    \"\"\"\n",
    "    #log_dir = os.path.join(folder_path, folder_name)\n",
    "    writer = SummaryWriter(log_dir=folder_path)\n",
    "    return writer\n",
    "\n",
    "\n",
    "def load_graph_or_image_data(row, target, input_type, model_type):\n",
    "    if model_type == 'GNN':\n",
    "        return load_graph_data(row, target, input_type)\n",
    "    elif model_type == 'CNN':\n",
    "        return load_image_data(row, target, input_type)\n",
    "\n",
    "def load_image_data(row, target, input_type, ):\n",
    "    \"\"\"\n",
    "    Load image (matrix) data for CNN based on the selected input type, along with the target.\n",
    "\n",
    "    Parameters:\n",
    "    - row: pd.Series, the row of the dataset containing paths to CIP, CM, and target.\n",
    "    - input_type: str, the type of input ('CIP', 'CM', or 'multi').\n",
    "    - target_column: str, the name of the column containing the target value.\n",
    "\n",
    "    Returns:\n",
    "    - The input data and the target.\n",
    "    \"\"\"\n",
    "    target = torch.tensor(row[target], dtype=torch.float)\n",
    "\n",
    "    if input_type == 'CIP':\n",
    "        cip_data = np.load(row['file_path_CIP'])\n",
    "        cip_data = torch.tensor(cip_data, dtype=torch.float)  # Shape (158, 158)\n",
    "        return cip_data, target  # Return both input and target\n",
    "\n",
    "    elif input_type == 'CM':\n",
    "        cm_data = np.load(row['file_path_CM'])\n",
    "        cm_data = torch.tensor(cm_data, dtype=torch.float)  # Shape (316, 316)\n",
    "        return cm_data, target  # Return both input and target\n",
    "\n",
    "    elif input_type == 'Multi':\n",
    "        cip_data = np.load(row['file_path_CIP'])\n",
    "        cm_data = np.load(row['file_path_CM'])\n",
    "        cip_data = torch.tensor(cip_data, dtype=torch.float)  # Shape (158, 158)\n",
    "        cm_data = torch.tensor(cm_data, dtype=torch.float)  # Shape (316, 316)\n",
    "        return cip_data, cm_data, target  # Return both inputs and the target\n",
    "\n",
    "\n",
    "def load_graph_data(row, target, input_type):\n",
    "    \"\"\"\n",
    "    Load graph data based on the selected input type.\n",
    "\n",
    "    Parameters:\n",
    "    - row: pd.Series, a row from the dataset.\n",
    "    - input_type: str, the input type ('CIP', 'CM', or 'Multi').\n",
    "    \"\"\"\n",
    "    if input_type == 'CIP':\n",
    "        # Load only CIP data\n",
    "        cip_data = np.load(row['file_path_CIP'])\n",
    "        edge_index_cip = torch.tensor([[i, j] for i in range(158) for j in range(158)], dtype=torch.long).t().contiguous()\n",
    "        x_cip = torch.tensor(cip_data, dtype=torch.float)\n",
    "        data_cip = Data(x=x_cip, edge_index=edge_index_cip, y=torch.tensor([row[target]], dtype=torch.float))\n",
    "        return data_cip\n",
    "\n",
    "    elif input_type == 'CM':\n",
    "        # Load only CM data\n",
    "        cm_data = np.load(row['file_path_CM'])\n",
    "        edge_index_cm = torch.tensor([[i, j] for i in range(316) for j in range(316)], dtype=torch.long).t().contiguous()\n",
    "        x_cm = torch.tensor(cm_data, dtype=torch.float)\n",
    "        data_cm = Data(x=x_cm, edge_index=edge_index_cm, y=torch.tensor([row[target]], dtype=torch.float))\n",
    "        return data_cm\n",
    "\n",
    "    elif input_type == 'Multi':\n",
    "        # Load both CIP and CM data\n",
    "        cip_data = np.load(row['file_path_CIP'])\n",
    "        cm_data = np.load(row['file_path_CM'])\n",
    "        edge_index_cip = torch.tensor([[i, j] for i in range(158) for j in range(158)], dtype=torch.long).t().contiguous()\n",
    "        edge_index_cm = torch.tensor([[i, j] for i in range(316) for j in range(316)], dtype=torch.long).t().contiguous()\n",
    "        x_cip = torch.tensor(cip_data, dtype=torch.float)\n",
    "        x_cm = torch.tensor(cm_data, dtype=torch.float)\n",
    "        data_cip = Data(x=x_cip, edge_index=edge_index_cip, y=torch.tensor([row[target]], dtype=torch.float))\n",
    "        data_cm = Data(x=x_cm, edge_index=edge_index_cm, y=torch.tensor([row[target]], dtype=torch.float))\n",
    "        return data_cip, data_cm\n",
    "\n",
    "def evaluate_model(data_loader, model, criterion, model_type, input_type):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for batch_idx, data in enumerate(data_loader):\n",
    "            if model_type == 'GNN':\n",
    "                if input_type == 'Multi':\n",
    "                    data_cip, data_cm = data\n",
    "\n",
    "                    data_cip = data_cip.to(device)\n",
    "                    data_cm = data_cm.to(device)\n",
    "\n",
    "                    output = model(data_cip, data_cm, input_type)\n",
    "                    loss = criterion(output, data_cip.y)\n",
    "                    mae = torch.nn.L1Loss()(output, data_cip.y)\n",
    "                else:\n",
    "                    data = data.to(device)\n",
    "\n",
    "                    if input_type == 'CIP':\n",
    "                        output = model(data, None, input_type)\n",
    "                    elif input_type == 'CM':\n",
    "                        output = model(None, data, input_type)\n",
    "\n",
    "                    loss = criterion(output, data.y)\n",
    "                    mae = torch.nn.L1Loss()(output, data.y)\n",
    "\n",
    "            elif model_type == 'CNN':\n",
    "                if input_type == 'Multi':\n",
    "                    data_cip, data_cm, target = data\n",
    "\n",
    "                    data_cip = data_cip.to(device)\n",
    "                    data_cm = data_cm.to(device)\n",
    "                    target = target.to(device)\n",
    "\n",
    "                    output = model(data_cip, data_cm, input_type)\n",
    "                else:\n",
    "                    data, target = data\n",
    "\n",
    "                    data = data.to(device)\n",
    "                    target = target.to(device)\n",
    "\n",
    "                    if input_type == 'CIP':\n",
    "                        output = model(data, None, input_type)\n",
    "                    elif input_type == 'CM':\n",
    "                        output = model(None, data, input_type)\n",
    "\n",
    "                # Ensure the target is passed as a tensor\n",
    "                loss = criterion(output, target.unsqueeze(1))  # Ensure target is the correct shape\n",
    "                mae = torch.nn.L1Loss()(output, target.unsqueeze(1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae.item()\n",
    "\n",
    "    avg_mse_loss = total_loss / len(data_loader)\n",
    "    avg_mae_loss = total_mae / len(data_loader)\n",
    "    return avg_mse_loss, avg_mae_loss\n",
    "\n",
    "def predict_model(data, model, model_type, input_type):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    if model_type == 'GNN':\n",
    "        if input_type == 'Multi':\n",
    "            data_cip, data_cm = data\n",
    "            data_cip = data_cip.to(device)\n",
    "            data_cm = data_cm.to(device)\n",
    "            output = model(data_cip, data_cm, input_type)\n",
    "            actual = data_cip.y\n",
    "        else:\n",
    "            data = data.to(device)\n",
    "            if input_type == 'CIP':\n",
    "                output = model(data, None, input_type)\n",
    "            elif input_type == 'CM':\n",
    "                output = model(None, data, input_type)\n",
    "            actual = data.y\n",
    "    elif model_type == 'CNN':\n",
    "        if input_type == 'Multi':\n",
    "            data_cip, data_cm, actual = data\n",
    "            data_cip = data_cip.to(device)\n",
    "            data_cm = data_cm.to(device)\n",
    "            #actual = actual.to(device)\n",
    "            output = model(data_cip, data_cm, input_type)\n",
    "        else:\n",
    "            data, actual = data\n",
    "            data = data.to(device)\n",
    "            #actual = actual.to(device)\n",
    "            if input_type == 'CIP':\n",
    "                output = model(data, None, input_type)\n",
    "            elif input_type == 'CM':\n",
    "                output = model(None, data, input_type)\n",
    "    return output, actual\n",
    "\n",
    "\n",
    "def load_model(folder_path):\n",
    "    #model = torch.load(f\"{folder_path}/model.pt\")\n",
    "    #model = torch.load(f\"{folder_path}/best_model.pt\")\n",
    "    model = torch.load(folder_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class MultiInputModel(torch.nn.Module):\n",
    "    def __init__(self, model_type, input_dim_cip, input_dim_cm, input_type,\n",
    "                 layer_configs_cip=None, layer_configs_cm=None):\n",
    "        super(MultiInputModel, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.input_type = input_type\n",
    "\n",
    "        # Default layer configurations if none are provided\n",
    "        if layer_configs_cip is None:\n",
    "            layer_configs_cip = [(32, 3, 0.5)] if model_type == 'CNN' else [(128, 0.5)]  # (channels, kernel_size, dropout) for CNN or (channels, dropout) for GNN\n",
    "        if layer_configs_cm is None:\n",
    "            layer_configs_cm = [(32, 3, 0.5)] if model_type == 'CNN' else [(128, 0.5)]\n",
    "\n",
    "        # Define model layers based on the type (CNN or GNN)\n",
    "        if model_type == 'CNN':\n",
    "            self.setup_cnn_layers(input_dim_cip, input_dim_cm, layer_configs_cip, layer_configs_cm)\n",
    "        elif model_type == 'GNN':\n",
    "            self.setup_gnn_layers(input_dim_cip, input_dim_cm, layer_configs_cip, layer_configs_cm)\n",
    "\n",
    "    def setup_cnn_layers(self, input_dim_cip, input_dim_cm, layer_configs_cip, layer_configs_cm):\n",
    "        self.layers_cip = torch.nn.ModuleList()\n",
    "        self.layers_cm = torch.nn.ModuleList()\n",
    "\n",
    "        # Set up layers for CIP\n",
    "        prev_channels_cip = 1\n",
    "        for out_channels, kernel_size, dropout in layer_configs_cip:\n",
    "            self.layers_cip.append(Conv2d(prev_channels_cip, out_channels, kernel_size=kernel_size, stride=1, padding=kernel_size//2))\n",
    "            self.layers_cip.append(Dropout(dropout))\n",
    "            prev_channels_cip = out_channels\n",
    "\n",
    "        # Set up layers for CM\n",
    "        prev_channels_cm = 1\n",
    "        for out_channels, kernel_size, dropout in layer_configs_cm:\n",
    "            self.layers_cm.append(Conv2d(prev_channels_cm, out_channels, kernel_size=kernel_size, stride=1, padding=kernel_size//2))\n",
    "            self.layers_cm.append(Dropout(dropout))\n",
    "            prev_channels_cm = out_channels\n",
    "\n",
    "        self.fc_cip = Linear(prev_channels_cip, 1)  # Assuming output from global pool\n",
    "        self.fc_cm = Linear(prev_channels_cm, 1)\n",
    "        if self.input_type == 'Multi':\n",
    "            self.fc = Linear(prev_channels_cip+prev_channels_cm, 1)\n",
    "\n",
    "    def setup_gnn_layers(self, input_dim_cip, input_dim_cm, layer_configs_cip, layer_configs_cm):\n",
    "        self.layers_cip = torch.nn.ModuleList()\n",
    "        self.layers_cm = torch.nn.ModuleList()\n",
    "\n",
    "        # Set up layers for CIP\n",
    "        prev_channels_cip = input_dim_cip\n",
    "        for out_channels, dropout in layer_configs_cip:\n",
    "            self.layers_cip.append(GCNConv(prev_channels_cip, out_channels))\n",
    "            self.layers_cip.append(Dropout(dropout))\n",
    "            prev_channels_cip = out_channels\n",
    "\n",
    "        # Set up layers for CM\n",
    "        prev_channels_cm = input_dim_cm\n",
    "        for out_channels, dropout in layer_configs_cm:\n",
    "            self.layers_cm.append(GCNConv(prev_channels_cm, out_channels))\n",
    "            self.layers_cm.append(Dropout(dropout))\n",
    "            prev_channels_cm = out_channels\n",
    "\n",
    "        self.fc_cip = Linear(prev_channels_cip, 1)\n",
    "        self.fc_cm = Linear(prev_channels_cm, 1)\n",
    "        if self.input_type == 'Multi':\n",
    "            self.fc = Linear(prev_channels_cip+prev_channels_cm, 1)\n",
    "\n",
    "    def forward(self, data_cip, data_cm, input_type):\n",
    "        if self.model_type == 'CNN':\n",
    "            return self.forward_cnn(data_cip, data_cm, input_type)\n",
    "        elif self.model_type == 'GNN':\n",
    "            return self.forward_gnn(data_cip, data_cm, input_type)\n",
    "\n",
    "    def forward_cnn(self, data_cip, data_cm, input_type):\n",
    "        if self.input_type in ['CIP', 'Multi']:\n",
    "            x = data_cip.unsqueeze(1)  # Adding channel dimension\n",
    "            for layer in self.layers_cip:\n",
    "                x = layer(x) if isinstance(layer, Conv2d) else F.relu(x)\n",
    "            x_cip = AdaptiveAvgPool2d((1, 1))(x).view(x.size(0), -1)\n",
    "\n",
    "        if self.input_type in ['CM', 'Multi']:\n",
    "            x = data_cm.unsqueeze(1)\n",
    "            for layer in self.layers_cm:\n",
    "                x = layer(x) if isinstance(layer, Conv2d) else F.relu(x)\n",
    "            x_cm = AdaptiveAvgPool2d((1, 1))(x).view(x.size(0), -1)\n",
    "\n",
    "        if self.input_type == 'CIP':\n",
    "            return self.fc_cip(x_cip)\n",
    "        elif self.input_type == 'CM':\n",
    "            return self.fc_cm(x_cm)\n",
    "        elif self.input_type == 'Multi':\n",
    "            x = torch.cat([x_cip, x_cm], dim=1)\n",
    "            return self.fc(x)\n",
    "\n",
    "    def forward_gnn(self, data_cip, data_cm, input_type):\n",
    "        if self.input_type in ['CIP', 'Multi']:\n",
    "            x = data_cip.x\n",
    "            edge_index = data_cip.edge_index\n",
    "            for layer in self.layers_cip:\n",
    "                x = layer(x, edge_index) if isinstance(layer, GCNConv) else F.relu(layer(x))\n",
    "            x_cip = global_mean_pool(x, data_cip.batch)\n",
    "\n",
    "        if self.input_type in ['CM', 'Multi']:\n",
    "            x = data_cm.x\n",
    "            edge_index = data_cm.edge_index\n",
    "            for layer in self.layers_cm:\n",
    "                x = layer(x, edge_index) if isinstance(layer, GCNConv) else F.relu(layer(x))\n",
    "            x_cm = global_mean_pool(x, data_cm.batch)\n",
    "\n",
    "        if self.input_type == 'CIP':\n",
    "            return self.fc_cip(x_cip)\n",
    "        elif self.input_type == 'CM':\n",
    "            return self.fc_cm(x_cm)\n",
    "        elif self.input_type == 'Multi':\n",
    "            x = torch.cat([x_cip, x_cm], dim=1)\n",
    "            return self.fc(x)\n",
    "\n",
    "\n",
    "def train_model(train_loader, test_loader, model, optimizer, criterion, model_type, input_type,\n",
    "                epochs, folder_name, folder_path):\n",
    "    \"\"\"\n",
    "    Train the GNN model.\n",
    "\n",
    "    Parameters:\n",
    "    - train_loader: DataLoader, the training data loader.\n",
    "    - model: torch.nn.Module, the CNN or GNN model.\n",
    "    - optimizer: torch.optim.Optimizer, the optimizer.\n",
    "    - criterion: torch.nn.Module, the loss function.\n",
    "    - model_type: str, the model type ('CNN', or 'GNN').\n",
    "    - input_type: str, the input type ('CIP', 'CM', or 'Multi').\n",
    "    - epochs: int, the number of training epochs.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    writer = setup_tensorboard_log(folder_name, folder_path)\n",
    "    best_mae = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss = 0\n",
    "        total_test_loss = 0\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            if model_type == 'GNN':\n",
    "                if input_type == 'Multi':\n",
    "                    data_cip, data_cm = data\n",
    "\n",
    "                    data_cip = data_cip.to(device)\n",
    "                    data_cm = data_cm.to(device)\n",
    "\n",
    "                    output = model(data_cip, data_cm, input_type)\n",
    "                    loss = criterion(output, data_cip.y)\n",
    "                else:\n",
    "                    data = data.to(device)\n",
    "\n",
    "                    if input_type == 'CIP':\n",
    "                        output = model(data, None, input_type)\n",
    "                    elif input_type == 'CM':\n",
    "                        output = model(None, data, input_type)\n",
    "\n",
    "                    loss = criterion(output, data.y)\n",
    "\n",
    "            elif model_type == 'CNN':\n",
    "                if input_type == 'Multi':\n",
    "                    data_cip, data_cm, target = data\n",
    "\n",
    "                    data_cip = data_cip.to(device)\n",
    "                    data_cm = data_cm.to(device)\n",
    "                    target = target.to(device)\n",
    "\n",
    "                    output = model(data_cip, data_cm, input_type)\n",
    "                else:\n",
    "                    data, target = data\n",
    "\n",
    "                    data = data.to(device)\n",
    "                    target = target.to(device)\n",
    "\n",
    "                    if input_type == 'CIP':\n",
    "                        output = model(data, None, input_type)\n",
    "                    elif input_type == 'CM':\n",
    "                        output = model(None, data, input_type)\n",
    "                # Ensure the target is passed as a tensor\n",
    "                loss = criterion(output, target.unsqueeze(1))  # Ensure target is the correct shape\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Train evaluation\n",
    "        mse_train_loss, mae_train_loss = evaluate_model(train_loader, model, criterion, model_type, input_type)\n",
    "        # Test evaluation\n",
    "        mse_test_loss, mae_test_loss = evaluate_model(test_loader, model, criterion, model_type, input_type)\n",
    "\n",
    "        # Log both training and test loss to TensorBoard\n",
    "        writer.add_scalar('Loss/Train/AVG_MSE/', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Train/MSE/', mse_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Train/MAE/', mae_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Test/MSE/', mse_test_loss, epoch)\n",
    "        writer.add_scalar('Loss/Test/MAE/', mae_test_loss, epoch)\n",
    "\n",
    "        # Save results to CSV\n",
    "        save_results_to_csv(folder_name, folder_path, epoch, avg_train_loss, mse_train_loss, mae_train_loss, mse_test_loss, mae_test_loss)\n",
    "\n",
    "        # Print results for the current epoch\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss}, MSE Train Loss: {mse_train_loss}, MAE Train Loss: {mae_train_loss}, MSE Test Loss: {mse_test_loss}, MAE Test Loss: {mae_test_loss}\")\n",
    "        # Save the model\n",
    "        torch.save(model, f\"{folder_path}/model.pt\")\n",
    "        if mae_test_loss <= best_mae:\n",
    "            best_mae = mae_test_loss\n",
    "            # Replace the best model\n",
    "            torch.save(model, f\"{folder_path}/best_model.pt\")\n",
    "            print(f\"Save the best model at Epoch [{epoch+1}/{epochs}]\")\n",
    "\n",
    "    writer.close()\n",
    "    # Save the model\n",
    "    torch.save(model, f\"{folder_path}/model.pt\")\n",
    "\n",
    "\n",
    "def initialize_model(model_type, input_dim_cip, input_dim_cm, input_type, save_path):\n",
    "    \"\"\"\n",
    "    Initialize a model based on the specified type and input dimensions.\n",
    "\n",
    "    Args:\n",
    "    model_type (str): 'CNN' or 'GNN' indicating the type of model.\n",
    "    input_dim_cip (int): Dimension size for CIP input.\n",
    "    input_dim_cm (int): Dimension size for CM input.\n",
    "    input_type (str): 'CIP', 'CM', or 'Multi' indicating the type of input handling.\n",
    "\n",
    "    Returns:\n",
    "    torch.nn.Module: Configured model instance.\n",
    "    \"\"\"\n",
    "    # Define default configurations for CNN and GNN for both CIP and CM inputs\n",
    "    if model_type == 'CNN':\n",
    "        # Define the configurations for CNN layers for both CIP and CM inputs\n",
    "        layer_configs_cip = [\n",
    "            (32, 3, 0.5),  # 32 filters, kernel size 3, dropout 0.1\n",
    "            (64, 3, 0.4),  # 64 filters, kernel size 3, dropout 0.1\n",
    "            (128, 3, 0.3),\n",
    "            (256, 3, 0.2),\n",
    "            (128, 3, 0.1)  # 128 filters, kernel size 3, dropout 0.1\n",
    "        ]\n",
    "        layer_configs_cm = [\n",
    "            (32, 3, 0.2),  # 32 filters, kernel size 3, dropout 0.2\n",
    "            (64, 3, 0.2)   # 64 filters, kernel size 3, dropout 0.2\n",
    "\n",
    "        ]\n",
    "\n",
    "    elif model_type == 'GNN':\n",
    "        # Define the configurations for GNN layers for both CIP and CM inputs\n",
    "        layer_configs_cip = [\n",
    "            (32, 0.5),  # 64 channels, dropout 0.5\n",
    "            (64, 0.4),  # 128 channels, dropout 0.4\n",
    "            (128, 0.3),\n",
    "            (256, 0.2),\n",
    "            (128, 0.1)  # 128 channels, dropout 0.1\n",
    "        ]\n",
    "        layer_configs_cm = [\n",
    "            (64, 0.2),  # 64 channels, dropout 0.2\n",
    "            (128, 0.2)  # 128 channels, dropout 0.2\n",
    "        ]\n",
    "\n",
    "\n",
    "    # Initialize the model with the specified configurations\n",
    "    model = MultiInputModel(\n",
    "            model_type=model_type,\n",
    "            input_dim_cip=input_dim_cip,\n",
    "            input_dim_cm=input_dim_cm,\n",
    "            input_type=input_type,\n",
    "            layer_configs_cip=layer_configs_cip,\n",
    "            layer_configs_cm=layer_configs_cm)\n",
    "\n",
    "    # Save model specifications to a JSON file\n",
    "    spec_path = os.path.join(save_path, 'model_spec.json')\n",
    "    specs = {\n",
    "        \"model_type\": model_type,\n",
    "        \"input_dim_cip\": input_dim_cip,\n",
    "        \"input_dim_cm\": input_dim_cm,\n",
    "        \"input_type\": input_type,\n",
    "        \"layer_configs_cip\": layer_configs_cip,\n",
    "        \"layer_configs_cm\": layer_configs_cm\n",
    "    }\n",
    "    with open(spec_path, 'w') as f:\n",
    "        json.dump(specs, f, indent=4)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf691a",
   "metadata": {
    "id": "fdbf691a"
   },
   "source": [
    "# Hyperparameters, info loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8fd3f",
   "metadata": {
    "id": "cfd8fd3f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "base_folder = './Data1/'\n",
    "target = 'target_HOMO' #'target_log_abs_HOMO' 'target_HOMO'\n",
    "input_type='CM' #'CIP' CM' 'Multi'\n",
    "test_material=None\n",
    "SEED=SEED\n",
    "epsilon=1e-5\n",
    "model_type='CNN'\n",
    "input_dim_cip=158\n",
    "input_dim_cm=316\n",
    "epochs=1000\n",
    "lr=0.001\n",
    "batch_size=64 #256\n",
    "shuffle=True\n",
    "log_path='./logs'\n",
    "\n",
    "# Switch between input types ('CIP', 'CM', 'multi')\n",
    "train_data, test_data = get_data(base_folder, target, input_type,\n",
    "                                 test_material, SEED, epsilon) # test_material=None 'DMSO1'\n",
    "\n",
    "train_data.head(5)\n",
    "\n",
    "# You can call this function after you load your train and test data\n",
    "analyze_target_distribution(train_data, test_data , target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52410d79",
   "metadata": {
    "id": "52410d79"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44479a6",
   "metadata": {
    "id": "b44479a6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the unique folder for logging\n",
    "folder_name, folder_path = create_unique_folder(log_path, model_type, input_type, target, test_material,\n",
    "                                   epochs, lr, batch_size, SEED)\n",
    "\n",
    "# Create the model\n",
    "model = initialize_model(model_type, input_dim_cip, input_dim_cm, input_type, folder_path)\n",
    "print(model)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "# Prepare data loader\n",
    "train_graphs = [load_graph_or_image_data(row, target, input_type, model_type) for _, row in train_data.iterrows()]\n",
    "test_graphs = [load_graph_or_image_data(row, target, input_type, model_type) for _, row in test_data.iterrows()]\n",
    "train_loader = DataLoader(train_graphs, batch_size, shuffle, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_graphs, batch_size, shuffle, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Train the model\n",
    "train_model(train_loader, test_loader, model, optimizer, criterion, model_type, input_type,\n",
    "            epochs, folder_name, folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086b12eb",
   "metadata": {
    "id": "086b12eb"
   },
   "source": [
    "# Loading Model and Post-training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b81ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path= './dir/model.pt'\n",
    "\n",
    "test_material=None\n",
    "epsilon=1e-5\n",
    "features = extract_features_from_path(model_path)\n",
    "train_data, test_data = get_data(base_folder, features['target'], features['input_type'], \n",
    "                                 test_material, features['SEED'], epsilon)\n",
    "train_graphs = [load_graph_or_image_data(row, features['target'], features['input_type'], features['model_type']) for _, row in train_data.iterrows()]\n",
    "test_graphs = [load_graph_or_image_data(row, features['target'], features['input_type'], features['model_type']) for _, row in test_data.iterrows()]\n",
    "train_loader = DataLoader(train_graphs, batch_size, shuffle, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_graphs, batch_size, shuffle, num_workers=4, pin_memory=True)\n",
    "\n",
    "print('Results on Train Set:')\n",
    "plot_predictions(train_loader, model_path, epsilon)\n",
    "print('\\n Results on Test Set:')\n",
    "plot_predictions(test_loader, model_path, epsilon)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
